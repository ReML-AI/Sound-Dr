{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook to keep a benchmarking result of Table 7 on Covid-19 and Abnormal Detection task, include Unsupervised(Isolation Forest, XGBOD) at last this notebook.\n",
    "\n",
    "+ Use Coswara dataset ```dataset_type='Coswara'```:\n",
    "+ Use FRILL pretrain model to extract Feature ```PRETRAIN=\"FRILL\"```\n",
    "+ Use SVM method to classify\n",
    "+ Set seed 1111 ```seed=1111```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoX could not be found!\n",
      "\n",
      "    If you do not have SoX, proceed here:\n",
      "     - - - http://sox.sourceforge.net/ - - -\n",
      "\n",
      "    If you do (or think that you should) have SoX, double-check your\n",
      "    path variables.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from glob import glob\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os, time, math, random, cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile, pickle, h5py, joblib, json\n",
    "import multiprocessing\n",
    "\n",
    "import librosa\n",
    "import opensmile\n",
    "import xgboost as xgb\n",
    "# import tensorflow_hub as hub\n",
    "\n",
    "from math import pi\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from scipy.fftpack import fft, hilbert\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict, train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score, auc, precision_recall_curve, roc_curve, average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 161.02it/s]\n",
      "100%|██████████| 64/64 [00:00<00:00, 161.89it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 81/81 [00:00<00:00, 161.78it/s]\n",
      "100%|██████████| 196/196 [00:01<00:00, 152.76it/s]\n",
      "100%|██████████| 66/66 [00:00<00:00, 164.74it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 152.50it/s]\n",
      "100%|██████████| 23/23 [00:00<00:00, 161.40it/s]\n",
      "100%|██████████| 168/168 [00:01<00:00, 163.79it/s]\n",
      "100%|██████████| 82/82 [00:00<00:00, 157.81it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 149.02it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 153.84it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 157.66it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 163.61it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 162.67it/s]\n",
      "100%|██████████| 56/56 [00:00<00:00, 161.37it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 156.63it/s]\n",
      "100%|██████████| 37/37 [00:00<00:00, 159.09it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 157.03it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 143.92it/s]\n",
      "100%|██████████| 76/76 [00:00<00:00, 150.58it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 29/29 [00:00<00:00, 147.44it/s]\n",
      "100%|██████████| 26/26 [00:00<00:00, 167.25it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 162.51it/s]\n",
      "100%|██████████| 31/31 [00:00<00:00, 163.84it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 162.85it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 161/161 [00:00<00:00, 161.81it/s]\n",
      "100%|██████████| 35/35 [00:00<00:00, 164.20it/s]\n",
      "100%|██████████| 21/21 [00:00<00:00, 164.47it/s]\n",
      "100%|██████████| 67/67 [00:00<00:00, 142.02it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 164.89it/s]\n",
      "100%|██████████| 54/54 [00:00<00:00, 139.30it/s]\n",
      "100%|██████████| 52/52 [00:00<00:00, 157.37it/s]\n",
      "100%|██████████| 83/83 [00:00<00:00, 165.69it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 155/155 [00:00<00:00, 164.25it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 155.09it/s]\n",
      "100%|██████████| 27/27 [00:00<00:00, 164.23it/s]\n",
      "100%|██████████| 41/41 [00:00<00:00, 167.57it/s]\n",
      "100%|██████████| 28/28 [00:00<00:00, 162.94it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 29/29 [00:00<00:00, 157.95it/s]\n",
      "100%|██████████| 46/46 [00:00<00:00, 150.37it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 165.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>DIR</th>\n",
       "      <th>a</th>\n",
       "      <th>covid_status</th>\n",
       "      <th>record_date</th>\n",
       "      <th>ep</th>\n",
       "      <th>g</th>\n",
       "      <th>l_c</th>\n",
       "      <th>l_l</th>\n",
       "      <th>l_s</th>\n",
       "      <th>...</th>\n",
       "      <th>st</th>\n",
       "      <th>ihd</th>\n",
       "      <th>asthma</th>\n",
       "      <th>others_preexist</th>\n",
       "      <th>cld</th>\n",
       "      <th>pneumonia</th>\n",
       "      <th>label_cough</th>\n",
       "      <th>file_path</th>\n",
       "      <th>label_covid</th>\n",
       "      <th>label_abnormal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d7w3B2YcJ3TLx58ryhiASEtwaAu1</td>\n",
       "      <td>./coswara_data/20210930/d7w3B2YcJ3TLx58ryhiASE...</td>\n",
       "      <td>72</td>\n",
       "      <td>positive_moderate</td>\n",
       "      <td>2021-09-29</td>\n",
       "      <td>y</td>\n",
       "      <td>male</td>\n",
       "      <td>India</td>\n",
       "      <td>Coimbatore</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>20210930/d7w3B2YcJ3TLx58ryhiASEtwaAu1/cough-sh...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JBt2sizAUqdzxaoHx2jCMl9ky1H2</td>\n",
       "      <td>./coswara_data/20210930/JBt2sizAUqdzxaoHx2jCMl...</td>\n",
       "      <td>73</td>\n",
       "      <td>healthy</td>\n",
       "      <td>2021-09-23</td>\n",
       "      <td>n</td>\n",
       "      <td>female</td>\n",
       "      <td>India</td>\n",
       "      <td>Coimbatore</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>20210930/JBt2sizAUqdzxaoHx2jCMl9ky1H2/cough-sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0Js6ZUZQ9NUnu568Fh7B6mZ1R8o1</td>\n",
       "      <td>./coswara_data/20210930/0Js6ZUZQ9NUnu568Fh7B6m...</td>\n",
       "      <td>50</td>\n",
       "      <td>positive_moderate</td>\n",
       "      <td>2021-09-24</td>\n",
       "      <td>y</td>\n",
       "      <td>female</td>\n",
       "      <td>India</td>\n",
       "      <td>Coimbatore</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>20210930/0Js6ZUZQ9NUnu568Fh7B6mZ1R8o1/cough-sh...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wRIACcPu2dS0xiHj6O7O4eJXKT53</td>\n",
       "      <td>./coswara_data/20210930/wRIACcPu2dS0xiHj6O7O4e...</td>\n",
       "      <td>31</td>\n",
       "      <td>positive_mild</td>\n",
       "      <td>2021-09-23</td>\n",
       "      <td>y</td>\n",
       "      <td>female</td>\n",
       "      <td>India</td>\n",
       "      <td>Coimbatore</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>20210930/wRIACcPu2dS0xiHj6O7O4eJXKT53/cough-sh...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8Ul16g4L9nP9lcp16BKh1X6cfhb2</td>\n",
       "      <td>./coswara_data/20210930/8Ul16g4L9nP9lcp16BKh1X...</td>\n",
       "      <td>46</td>\n",
       "      <td>positive_moderate</td>\n",
       "      <td>2021-09-25</td>\n",
       "      <td>y</td>\n",
       "      <td>female</td>\n",
       "      <td>India</td>\n",
       "      <td>Coimbatore</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>20210930/8Ul16g4L9nP9lcp16BKh1X6cfhb2/cough-sh...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id  \\\n",
       "0  d7w3B2YcJ3TLx58ryhiASEtwaAu1   \n",
       "0  JBt2sizAUqdzxaoHx2jCMl9ky1H2   \n",
       "0  0Js6ZUZQ9NUnu568Fh7B6mZ1R8o1   \n",
       "0  wRIACcPu2dS0xiHj6O7O4eJXKT53   \n",
       "0  8Ul16g4L9nP9lcp16BKh1X6cfhb2   \n",
       "\n",
       "                                                 DIR   a       covid_status  \\\n",
       "0  ./coswara_data/20210930/d7w3B2YcJ3TLx58ryhiASE...  72  positive_moderate   \n",
       "0  ./coswara_data/20210930/JBt2sizAUqdzxaoHx2jCMl...  73            healthy   \n",
       "0  ./coswara_data/20210930/0Js6ZUZQ9NUnu568Fh7B6m...  50  positive_moderate   \n",
       "0  ./coswara_data/20210930/wRIACcPu2dS0xiHj6O7O4e...  31      positive_mild   \n",
       "0  ./coswara_data/20210930/8Ul16g4L9nP9lcp16BKh1X...  46  positive_moderate   \n",
       "\n",
       "  record_date ep       g    l_c         l_l         l_s  ...    st  ihd  \\\n",
       "0  2021-09-29  y    male  India  Coimbatore  Tamil Nadu  ...  True  NaN   \n",
       "0  2021-09-23  n  female  India  Coimbatore  Tamil Nadu  ...   NaN  NaN   \n",
       "0  2021-09-24  y  female  India  Coimbatore  Tamil Nadu  ...   NaN  NaN   \n",
       "0  2021-09-23  y  female  India  Coimbatore  Tamil Nadu  ...  True  NaN   \n",
       "0  2021-09-25  y  female  India  Coimbatore  Tamil Nadu  ...   NaN  NaN   \n",
       "\n",
       "  asthma others_preexist  cld pneumonia label_cough  \\\n",
       "0    NaN             NaN  NaN       NaN           1   \n",
       "0    NaN             NaN  NaN       NaN           0   \n",
       "0    NaN             NaN  NaN       NaN           0   \n",
       "0    NaN             NaN  NaN       NaN           1   \n",
       "0    NaN             NaN  NaN       NaN           0   \n",
       "\n",
       "                                           file_path  label_covid  \\\n",
       "0  20210930/d7w3B2YcJ3TLx58ryhiASEtwaAu1/cough-sh...            1   \n",
       "0  20210930/JBt2sizAUqdzxaoHx2jCMl9ky1H2/cough-sh...            0   \n",
       "0  20210930/0Js6ZUZQ9NUnu568Fh7B6mZ1R8o1/cough-sh...            1   \n",
       "0  20210930/wRIACcPu2dS0xiHj6O7O4eJXKT53/cough-sh...            1   \n",
       "0  20210930/8Ul16g4L9nP9lcp16BKh1X6cfhb2/cough-sh...            1   \n",
       "\n",
       "  label_abnormal  \n",
       "0              1  \n",
       "0              0  \n",
       "0              0  \n",
       "0              1  \n",
       "0              0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample rate\n",
    "SR = 44100\n",
    "#100 ms\n",
    "FRAME_LEN = int(SR / 10)\n",
    "#50% overlap, meaning 5ms hop length\n",
    "HOP = int(FRAME_LEN / 2)\n",
    "#the MFCC dimension\n",
    "MFCC_dim = 13\n",
    "PRETRAIN = 'FRILL'\n",
    "codebook_size = 1000\n",
    "\n",
    "fold_num = 5\n",
    "seed = 1111\n",
    "\n",
    "dataset_type = 'Coswara'\n",
    "\n",
    "if dataset_type == 'SoundDr':\n",
    "    PERIOD = 15\n",
    "    SR = 48000\n",
    "    DIR_DATA = \"./sounddr_data/\"\n",
    "\n",
    "    df = pd.read_csv(DIR_DATA + 'data.csv')\n",
    "    df['file_path'] = df['file_cough'] + '.wav'\n",
    "\n",
    "    df['label_symptom'] = (df['symptoms_status_choice'].map(str) != \"['No']\").astype(int)\n",
    "    df['label_abnormal'] = ((df['symptoms_status_choice'].map(str) != \"['No']\") | (df['cov19_status_choice'] != 'never')).astype(int)\n",
    "    df['label_covid'] = (df['cov19_status_choice'] != 'never').astype(int)\n",
    "elif dataset_type == 'CoughVid':\n",
    "    PERIOD = 10\n",
    "    SR = 22050\n",
    "\n",
    "    DIR_DATA = './coughvid_data/'\n",
    "\n",
    "    VidData   = pd.read_csv(os.path.join(DIR_DATA, 'public_dataset/metadata_compiled.csv'), header=0)\n",
    "    VidData   = VidData.loc[VidData['cough_detected'] >= 0.9][['uuid','fever_muscle_pain','respiratory_condition','status', 'quality_1', 'age', 'gender']]\n",
    "    VidData.dropna(subset=['uuid','fever_muscle_pain','respiratory_condition','status'], inplace=True)\n",
    "    VidData = VidData[(VidData['quality_1'] != 'no_cough') & (VidData['quality_1'] != 'poor')]\n",
    "    VidData = VidData[(VidData['status'] != 'symptomatic') & (VidData['status'].notna())]\n",
    "    VidData['label_covid'] = (VidData['status'] == 'COVID-19').astype(int)\n",
    "\n",
    "    extradata = VidData.loc[VidData['status']=='COVID-19']\n",
    "    notradata = VidData.loc[VidData['status']!='COVID-19']\n",
    "\n",
    "    df = pd.concat([extradata, notradata], ignore_index= True)\n",
    "    df['file_path'] = df['uuid'].apply(lambda x: 'public_dataset/' + x + '.webm')\n",
    "    def g(x):\n",
    "        for i in x:\n",
    "            if i is True:\n",
    "                return 1\n",
    "        return 0\n",
    "    df['label_abnormal'] = df[['fever_muscle_pain', 'respiratory_condition', 'label_covid']].apply(lambda x: g(x), axis=1)\n",
    "else:\n",
    "    PERIOD = 5\n",
    "    SR = 44100\n",
    "    DIR_DATA = \"./coswara_data/\"\n",
    "\n",
    "    join_by = pd.read_csv(os.path.join(DIR_DATA, 'combined_data.csv'))\n",
    "    df_list = []\n",
    "    for each in os.listdir(DIR_DATA):\n",
    "        for path in tqdm(glob(DIR_DATA + each + '/*/cough-shallow.wav')):\n",
    "            temp = pd.DataFrame(columns=['id', 'DIR'])\n",
    "            temp['id'] = [path.split('/')[-2]]\n",
    "            temp['DIR'] = [path]\n",
    "            temp = pd.merge(left=temp,right=join_by,on='id',how='inner')\n",
    "\n",
    "            temp['label_cough'] = (temp['cough'] == True).astype(int)\n",
    "\n",
    "            temp['file_path'] = each + '/' + temp['id'] + '/cough-shallow.wav'\n",
    "            temp['label_covid'] = temp['covid_status'].apply(lambda x: 1 if x == 'positive_mild' or x =='positive_moderate' or x == 'COVID-19' else 0)\n",
    "            df_list.append(temp)\n",
    "    df = pd.concat(df_list)\n",
    "    def g(x):\n",
    "        for i in x:\n",
    "            if i is True:\n",
    "                return 1\n",
    "        return 0\n",
    "    df['label_abnormal'] = df[['st', 'bd', 'cld', 'pneumonia', 'others_resp', 'asthma', 'label_covid']].apply(lambda x: g(x), axis=1)\n",
    "\n",
    "target_col = 'label_abnormal'\n",
    "OUTPUT_DIR = DIR_DATA + 'output/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok = True)\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_duration(filename, mono=True, res_type=\"kaiser_fast\"):\n",
    "    duration = 0\n",
    "    sr = SR\n",
    "    try:\n",
    "        y, sr = librosa.load(filename, sr=None, mono=mono, res_type=res_type)\n",
    "        duration = librosa.get_duration(y=y, sr=sr)\n",
    "    except:\n",
    "        print('Error file:' + filename)\n",
    "    return duration, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1841\n",
       "1     391\n",
       "Name: label_abnormal, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[target_col].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def crop_or_pad(y, length):\n",
    "    if len(y) < length:\n",
    "        y = np.concatenate([y, np.zeros(length-len(y))])\n",
    "    elif len(y) > length:\n",
    "        cut = random.randint(0, len(y) - length)\n",
    "        y = y[cut:cut+length]\n",
    "    return y\n",
    "\n",
    "def merge_feature(list_features):\n",
    "    \"\"\"\n",
    "      Merge numpy array features\n",
    "      Args:\n",
    "        - list_features: list of numpy array features\n",
    "                         :type: a list of numpy arrays \n",
    "      Returns:\n",
    "        - features: the concatenate numpy array along axis=1\n",
    "                    :type: a numpy array                 \n",
    "    \"\"\"      \n",
    "    features = np.concatenate(list_features, axis=1)\n",
    "    features = np.nan_to_num(features)\n",
    "    features = np.clip(features, -np.finfo(np.float32).max, np.finfo(np.float32).max)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(cfs_matrix):\n",
    "    \"\"\"\n",
    "      Calculate common metrics based on the confusion matrix\n",
    "      Args:\n",
    "        - cfs_matrix: a sklearn confusion matrix \n",
    "                      :type: a ndarray of shape (n_classes, n_classes)\n",
    "      Returns:\n",
    "        - precision: the precision of the prediction\n",
    "                     :type: float  \n",
    "        - recall: the recall of the prediction\n",
    "                  :type: float  \n",
    "        - f1: the f1-score of the prediction\n",
    "              :type: float                       \n",
    "    \"\"\"     \n",
    "    precision = cfs_matrix[1,1] / (cfs_matrix[1,1] + cfs_matrix[0,1])\n",
    "    recall = cfs_matrix[1,1] / (cfs_matrix[1,1] + cfs_matrix[1,0])\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(OUTPUT_DIR + PRETRAIN + '.pickle'):\n",
    "    import tensorflow.compat.v2 as tf\n",
    "    tf.enable_v2_behavior()\n",
    "    import tensorflow_hub as hub\n",
    "\n",
    "    frill_nofrontend_model = hub.load('https://tfhub.dev/google/nonsemantic-speech-benchmark/frill-nofrontend/1')\n",
    "\n",
    "    def stabilized_log(data, additive_offset, floor):\n",
    "      \"\"\"TF version of mfcc_mel.StabilizedLog.\"\"\"\n",
    "      return tf.math.log(tf.math.maximum(data, floor) + additive_offset)\n",
    "\n",
    "\n",
    "    def log_mel_spectrogram(data,\n",
    "                            audio_sample_rate,\n",
    "                            num_mel_bins=64,\n",
    "                            log_additive_offset=0.001,\n",
    "                            log_floor=1e-12,\n",
    "                            window_length_secs=0.025,\n",
    "                            hop_length_secs=0.010,\n",
    "                            fft_length=None):\n",
    "        \"\"\"TF version of mfcc_mel.LogMelSpectrogram.\"\"\"\n",
    "        window_length_samples = int(round(audio_sample_rate * window_length_secs))\n",
    "        hop_length_samples = int(round(audio_sample_rate * hop_length_secs))\n",
    "        if not fft_length:\n",
    "            fft_length = 2 ** int(np.ceil(np.log(window_length_samples) / np.log(2.0)))\n",
    "\n",
    "        spectrogram = tf.abs(\n",
    "            tf.signal.stft(\n",
    "                tf.cast(data, tf.dtypes.float64),\n",
    "                frame_length=window_length_samples,\n",
    "                frame_step=hop_length_samples,\n",
    "                fft_length=fft_length,\n",
    "                window_fn=tf.signal.hann_window,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        to_mel = tf.signal.linear_to_mel_weight_matrix(\n",
    "            num_mel_bins=num_mel_bins,\n",
    "            num_spectrogram_bins=fft_length // 2 + 1,\n",
    "            sample_rate=audio_sample_rate,\n",
    "            lower_edge_hertz=125.0,\n",
    "            upper_edge_hertz=7500.0,\n",
    "            dtype=tf.dtypes.float64\n",
    "        )\n",
    "\n",
    "        mel = spectrogram @ to_mel\n",
    "        log_mel = stabilized_log(mel, log_additive_offset, log_floor)\n",
    "        return log_mel\n",
    "\n",
    "    def compute_frontend_features(samples, sr, frame_hop, n_required=16000, num_mel_bins=64, frame_width=96):\n",
    "        if samples.dtype == np.int16:\n",
    "            samples = tf.cast(samples, np.float32) / np.iinfo(np.int16).max\n",
    "        if samples.dtype == np.float64:\n",
    "            samples = tf.cast(samples, np.float32)\n",
    "        assert samples.dtype == np.float32, samples.dtype\n",
    "        n = tf.size(samples)\n",
    "        samples = tf.cond(\n",
    "            n < n_required,\n",
    "            lambda: tf.pad(samples, [(0, n_required - n)]),\n",
    "            lambda: samples\n",
    "        )\n",
    "        mel = log_mel_spectrogram(samples, sr, num_mel_bins=num_mel_bins)\n",
    "        mel = tf.signal.frame(mel, frame_length=frame_width, frame_step=frame_hop, axis=0)\n",
    "        return mel\n",
    "\n",
    "    def make_nonsemantic_frill_nofrontend_feat(filename):\n",
    "        try:\n",
    "            waveform, _ = librosa.load(os.path.join(DIR_DATA, filename), sr=16000, mono=True, res_type=\"kaiser_fast\")\n",
    "            if 2048 > waveform.shape[-1]:\n",
    "                print('File length < 2048')\n",
    "                return None, filename\n",
    "            frontend_feats = tf.expand_dims(compute_frontend_features(waveform, 16000, frame_hop=17), axis=-1).numpy().astype(np.float32)\n",
    "            assert frontend_feats.shape[1:] == (96, 64, 1)\n",
    "\n",
    "            embeddings = frill_nofrontend_model(frontend_feats)['embedding']\n",
    "            mean_emb = embeddings.numpy().mean(axis=0)\n",
    "            std_emb = embeddings.numpy().std(axis=0)\n",
    "        except Exception as e:\n",
    "            print('Error: ' + str(e))\n",
    "            return None, filename\n",
    "        return np.concatenate((mean_emb, std_emb)), filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_features_of_list_audio(df):\n",
    "    X_features = []\n",
    "    df['error'] = 0\n",
    "    # extract train data features\n",
    "#     pool = multiprocessing.Pool(multiprocessing.cpu_count())\n",
    "#     with tqdm(total=len(df.values)) as t:\n",
    "#         for feature, filename in pool.imap(make_nonsemantic_trill_feat, df['file_path'].values):\n",
    "#             if feature is None:\n",
    "#                 df['error'][df['file_path'] == filename] = 1\n",
    "#             else:\n",
    "#                 X_features.append(feature)\n",
    "#             t.update(1)\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "\n",
    "    for idx, r in tqdm(df.iterrows(), total=len(df)):\n",
    "        if PRETRAIN == 'TRILL':\n",
    "            feature, filename = make_nonsemantic_trill_feat(r['file_path'])\n",
    "        elif PRETRAIN == 'OpenSmileBoAW':\n",
    "            feature, filename = make_opensmileboaw_feat(r['file_path'])\n",
    "        elif PRETRAIN == 'OpenSmile':\n",
    "            feature, filename = make_opensmile_feat(r['file_path'])\n",
    "        elif PRETRAIN == 'DeepSpectrum':\n",
    "            feature, filename = make_deepspect_feat(r['file_path'])\n",
    "        else:\n",
    "            feature, filename = make_nonsemantic_frill_nofrontend_feat(r['file_path'])\n",
    "        \n",
    "        if feature is None:\n",
    "            df['error'][df['file_path'] == filename] = 1\n",
    "        else:\n",
    "            X_features.append(feature)\n",
    "    return np.array(X_features), df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data feature shape: ((2201, 4096), 2201)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(OUTPUT_DIR + PRETRAIN + '.pickle'):\n",
    "    X_features, df = get_features_of_list_audio(df)\n",
    "    df.to_csv(os.path.join(OUTPUT_DIR, 'data.csv'), index=False)\n",
    "    pickle.dump({\n",
    "        'X_trill_features': X_features\n",
    "    }, open(OUTPUT_DIR + PRETRAIN + '.pickle', \"wb\" ))\n",
    "else:\n",
    "    df = pd.read_csv(os.path.join(OUTPUT_DIR, 'data.csv'))\n",
    "    f = pickle.load(open(OUTPUT_DIR + PRETRAIN + '.pickle', \"rb\" ))\n",
    "    X_features = f['X_trill_features']\n",
    "df = df[df['error'] == 0].reset_index(drop=True)\n",
    "\n",
    "# if PRETRAIN == 'OpenSmile':\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(merge_feature([X_features]))\n",
    "# else:\n",
    "# X = merge_feature([X_features])\n",
    "print(f\"Data feature shape: {X.shape, len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(ensem_preds, targets):\n",
    "    \"\"\"\n",
    "      Evaluate the prediction by providing metrics & also the best threshold (to get the highest f1-score)\n",
    "      Ex: AUC, Accurary, Precision, Recall, F1-Score.\n",
    "      Then print these metrics\n",
    "      Args:\n",
    "        - ensem_preds: predictions for ids \n",
    "                       :type: a numpy array\n",
    "        - targets: the actual results of ids \n",
    "                   :type: a numpy array                 \n",
    "      Returns:\n",
    "        - None                  \n",
    "    \"\"\"     \n",
    "    best_th = 0\n",
    "    best_score = 0\n",
    "\n",
    "    for th in np.arange(0.0, 0.6, 0.01):\n",
    "        pred = (ensem_preds > th).astype(int)\n",
    "        score = f1_score(targets, pred)\n",
    "        if score > best_score:\n",
    "            best_th = th\n",
    "            best_score = score\n",
    "\n",
    "    print(f\"\\nAUC score: {roc_auc_score(targets, ensem_preds):12.4f}\")\n",
    "    print(f\"Best threshold {best_th:12.4f}\")\n",
    "\n",
    "    preds = (ensem_preds > best_th).astype(int)\n",
    "\n",
    "    cm1 = confusion_matrix(targets, preds)\n",
    "    print('\\nConfusion Matrix : \\n', cm1)\n",
    "    precision, recall, f1 = compute_metrics(cm1)\n",
    "    \n",
    "    print('\\n=============')\n",
    "    print (f'Precision    : {precision:12.4f}')\n",
    "    \n",
    "    print(f'Recall : {recall:12.4f}')\n",
    "    \n",
    "    print(f'F1 Score : {f1:12.4f}')\n",
    "    \n",
    "    total1=sum(sum(cm1))\n",
    "\n",
    "    print('\\n=============')\n",
    "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
    "    print (f'Accuracy    : {accuracy1:12.4f}')\n",
    "\n",
    "def get_model(pos_scale, c=1):\n",
    "    model = LinearSVC(C=c, class_weight='balanced', random_state=seed)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train COVID-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    " if not os.path.exists(OUTPUT_DIR + 'df_label_covid_5fold.csv'):\n",
    "    folds = df.copy()\n",
    "    Fold = StratifiedKFold(n_splits=fold_num, shuffle=True, random_state=seed)\n",
    "    for n, (train_index, val_index) in enumerate(Fold.split(folds, folds['label_covid'])):\n",
    "        folds.loc[val_index, 'fold'] = int(n)\n",
    "    folds['fold'] = folds['fold'].astype(int)\n",
    "    folds.to_csv(OUTPUT_DIR + 'df_label_covid_5fold.csv', index=False)\n",
    "else:\n",
    "    folds = pd.read_csv(OUTPUT_DIR + 'df_label_covid_5fold.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.822751322751323\n",
      "0.6382119682768566\n",
      "0.6547031963470321\n",
      "0.6974429223744292\n",
      "0.628903990746096\n",
      "0.6365673799884326\n",
      "(!) cv5 AUC  0.6511658915465692 0.024619459793158903\n",
      "\n",
      "AUC score:       0.6510\n",
      "Best threshold       0.0000\n",
      "\n",
      "Confusion Matrix : \n",
      " [[1467  356]\n",
      " [ 190  188]]\n",
      "\n",
      "=============\n",
      "Precision    :       0.3456\n",
      "Recall :       0.4974\n",
      "F1 Score :       0.4078\n",
      "\n",
      "=============\n",
      "Accuracy    :       0.7519\n"
     ]
    }
   ],
   "source": [
    "y = folds['label_covid']\n",
    "pos_scale = (y == 0).sum() / (y == 1).sum()\n",
    "print(pos_scale)\n",
    "targets = []\n",
    "preds = []\n",
    "aucs = []\n",
    "\n",
    "for fold in range(5):\n",
    "    train_idx = folds['fold'] != fold\n",
    "    valid_idx = folds['fold'] == fold\n",
    "    X_train = X[train_idx]\n",
    "    y_train = y[train_idx]\n",
    "    X_val = X[valid_idx]\n",
    "    y_val = y[valid_idx]\n",
    "\n",
    "    targets.append(y_val)\n",
    "    model = get_model(pos_scale=pos_scale)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_val)\n",
    "    preds.append(pred)\n",
    "    auc = roc_auc_score(y_val, pred)\n",
    "    aucs.append(auc)\n",
    "    print(auc)\n",
    "    del model\n",
    "\n",
    "targets = np.concatenate(targets)\n",
    "preds = np.concatenate(preds)\n",
    "\n",
    "print(\"(!) cv5 AUC \", np.mean(aucs), np.std(aucs))\n",
    "evaluate(preds, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train abnormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    " if not os.path.exists(OUTPUT_DIR + 'df_label_abnormal_5fold.csv'):\n",
    "    folds = df.copy()\n",
    "    Fold = StratifiedKFold(n_splits=fold_num, shuffle=True, random_state=seed)\n",
    "    for n, (train_index, val_index) in enumerate(Fold.split(folds, folds['label_abnormal'])):\n",
    "        folds.loc[val_index, 'fold'] = int(n)\n",
    "    folds['fold'] = folds['fold'].astype(int)\n",
    "    folds.to_csv(OUTPUT_DIR + 'df_label_abnormal_5fold.csv', index=False)\n",
    "else:\n",
    "    folds = pd.read_csv(OUTPUT_DIR + 'df_label_abnormal_5fold.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.658097686375322\n",
      "0.5915448188175461\n",
      "0.5905155450609996\n",
      "0.5887519478679699\n",
      "0.5768168295792605\n",
      "0.5667587476979743\n",
      "(!) cv5 AUC  0.58287757780475 0.009638642926106993\n",
      "\n",
      "AUC score:       0.5829\n",
      "Best threshold       0.0000\n",
      "\n",
      "Confusion Matrix : \n",
      " [[1763   49]\n",
      " [ 314   75]]\n",
      "\n",
      "=============\n",
      "Precision    :       0.6048\n",
      "Recall :       0.1928\n",
      "F1 Score :       0.2924\n",
      "\n",
      "=============\n",
      "Accuracy    :       0.8351\n"
     ]
    }
   ],
   "source": [
    "y = folds['label_abnormal']\n",
    "pos_scale = (y == 0).sum() / (y == 1).sum()\n",
    "print(pos_scale)\n",
    "targets = []\n",
    "preds = []\n",
    "aucs = []\n",
    "\n",
    "def get_model(pos_scale):\n",
    "    model = xgb.XGBClassifier(\n",
    "        max_depth=7,\n",
    "        scale_pos_weight=pos_scale,\n",
    "        learning_rate=0.3,\n",
    "        n_estimators=200,\n",
    "        subsample=1,\n",
    "        colsample_bytree=1,\n",
    "        nthread=-1,\n",
    "        seed=seed,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    return model\n",
    "\n",
    "for fold in range(5):\n",
    "    train_idx = folds['fold'] != fold\n",
    "    valid_idx = folds['fold'] == fold\n",
    "    X_train = X[train_idx]\n",
    "    y_train = y[train_idx]\n",
    "    X_val = X[valid_idx]\n",
    "    y_val = y[valid_idx]\n",
    "\n",
    "    targets.append(y_val)\n",
    "    model = get_model(pos_scale=pos_scale)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_val)\n",
    "    preds.append(pred)\n",
    "    auc = roc_auc_score(y_val, pred)\n",
    "    aucs.append(auc)\n",
    "    print(auc)\n",
    "    del model\n",
    "\n",
    "targets = np.concatenate(targets)\n",
    "preds = np.concatenate(preds)\n",
    "\n",
    "print(\"(!) cv5 AUC \", np.mean(aucs), np.std(aucs))\n",
    "evaluate(preds, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test other model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.658097686375322\n"
     ]
    }
   ],
   "source": [
    "y = folds['label_abnormal']\n",
    "pos_scale = (y == 0).sum() / (y == 1).sum()\n",
    "print(pos_scale)\n",
    "\n",
    "def get_model(pos_scale):\n",
    "    model = IsolationForest(n_estimators=500, max_samples='auto', contamination=0.1, n_jobs=-1, random_state=seed) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4778907960726142\n",
      "0.5172265750778148\n",
      "0.49247414647967136\n",
      "0.5215682107947301\n",
      "0.47565165037540724\n",
      "(!) cv5 AUC  0.4969622757600475 0.019256693703546882\n",
      "\n",
      "AUC score:       0.4958\n",
      "Best threshold       0.0000\n",
      "\n",
      "Confusion Matrix : \n",
      " [[1637  175]\n",
      " [ 337   52]]\n",
      "\n",
      "=============\n",
      "Precision    :       0.2291\n",
      "Recall :       0.1337\n",
      "F1 Score :       0.1688\n",
      "\n",
      "=============\n",
      "Accuracy    :       0.7674\n"
     ]
    }
   ],
   "source": [
    "targets = []\n",
    "preds = []\n",
    "aucs = []\n",
    "\n",
    "for fold in range(5):\n",
    "    train_idx = folds['fold'] != fold\n",
    "    valid_idx = folds['fold'] == fold\n",
    "    X_train = X[train_idx]\n",
    "    y_train = y[train_idx]\n",
    "    X_val = X[valid_idx]\n",
    "    y_val = y[valid_idx]\n",
    "\n",
    "    targets.append(y_val)\n",
    "    model = get_model(pos_scale)\n",
    "    model.fit(X_train)\n",
    "\n",
    "    scores = (-1.0) * model.decision_function(X_val)\n",
    "    pred = scores.flatten()\n",
    "    preds.append(pred)\n",
    "    auc = roc_auc_score(y_val, pred)\n",
    "    aucs.append(auc)\n",
    "    print(auc)\n",
    "    del model\n",
    "\n",
    "targets = np.concatenate(targets)\n",
    "preds = np.concatenate(preds)\n",
    "\n",
    "print(\"(!) cv5 AUC \", np.mean(aucs), np.std(aucs))\n",
    "evaluate(preds, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.658097686375322\n"
     ]
    }
   ],
   "source": [
    "y = folds['label_abnormal']\n",
    "pos_scale = (y == 0).sum() / (y == 1).sum()\n",
    "print(pos_scale)\n",
    "\n",
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "def get_model(pos_scale):\n",
    "    # Model candidate\n",
    "    model = XGBOD(max_depth=7,\n",
    "        scale_pos_weight=pos_scale,\n",
    "        learning_rate=0.3,\n",
    "        n_estimators=200,\n",
    "        subsample=1,\n",
    "        colsample_bytree=1,\n",
    "        nthread=-1,\n",
    "        seed=seed,\n",
    "        eval_metric='logloss')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:46:01] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "0.6851380942290033\n",
      "[01:58:06] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "0.6471324818432257\n",
      "[02:10:06] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "0.6778403456580252\n",
      "[02:22:08] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "0.6633729990083581\n",
      "[02:34:06] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "0.6987002408273126\n",
      "(!) cv5 AUC  0.674436832313185 0.017791851305506427\n",
      "\n",
      "AUC score:       0.6750\n",
      "Best threshold       0.0400\n",
      "\n",
      "Confusion Matrix : \n",
      " [[1503  309]\n",
      " [ 206  183]]\n",
      "\n",
      "=============\n",
      "Precision    :       0.3720\n",
      "Recall :       0.4704\n",
      "F1 Score :       0.4154\n",
      "\n",
      "=============\n",
      "Accuracy    :       0.7660\n"
     ]
    }
   ],
   "source": [
    "targets = []\n",
    "preds = []\n",
    "aucs = []\n",
    "\n",
    "for fold in range(5):\n",
    "    train_idx = folds['fold'] != fold\n",
    "    valid_idx = folds['fold'] == fold\n",
    "    X_train = X[train_idx]\n",
    "    y_train = y[train_idx]\n",
    "    X_val = X[valid_idx]\n",
    "    y_val = y[valid_idx]\n",
    "\n",
    "    targets.append(y_val)\n",
    "    model = get_model(pos_scale)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    pred = model.decision_function(X_val)  # predict raw outlier scores on test\n",
    "    auc = roc_auc_score(y_val, pred)\n",
    "    preds.append(pred)\n",
    "    aucs.append(auc)\n",
    "    print(auc)\n",
    "    del model\n",
    "\n",
    "targets = np.concatenate(targets)\n",
    "preds = np.concatenate(preds)\n",
    "\n",
    "print(\"(!) cv5 AUC \", np.mean(aucs), np.std(aucs))\n",
    "evaluate(preds, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.658097686375322\n"
     ]
    }
   ],
   "source": [
    "y = folds['label_abnormal']\n",
    "pos_scale = (y == 0).sum() / (y == 1).sum()\n",
    "print(pos_scale)\n",
    "\n",
    "from pyod.models.iforest import IForest\n",
    "\n",
    "def get_model(pos_scale):\n",
    "    model = IForest(n_estimators=500, max_samples='auto', contamination=0.1, n_jobs=-1, random_state=seed) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4778907960726142\n",
      "0.5172265750778148\n",
      "0.49247414647967136\n",
      "0.5215682107947301\n",
      "0.47565165037540724\n",
      "(!) cv5 AUC  0.4969622757600475 0.019256693703546882\n",
      "\n",
      "AUC score:       0.4958\n",
      "Best threshold       0.0000\n",
      "\n",
      "Confusion Matrix : \n",
      " [[1637  175]\n",
      " [ 337   52]]\n",
      "\n",
      "=============\n",
      "Precision    :       0.2291\n",
      "Recall :       0.1337\n",
      "F1 Score :       0.1688\n",
      "\n",
      "=============\n",
      "Accuracy    :       0.7674\n"
     ]
    }
   ],
   "source": [
    "targets = []\n",
    "preds = []\n",
    "aucs = []\n",
    "\n",
    "for fold in range(5):\n",
    "    train_idx = folds['fold'] != fold\n",
    "    valid_idx = folds['fold'] == fold\n",
    "    X_train = X[train_idx]\n",
    "    y_train = y[train_idx]\n",
    "    X_val = X[valid_idx]\n",
    "    y_val = y[valid_idx]\n",
    "\n",
    "    targets.append(y_val)\n",
    "    \n",
    "    if PRETRAIN == 'OpenSmileBoAW':\n",
    "        from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "        p = int(X_train.shape[1]/130)\n",
    "        X_train1 = X_train[:, :(p-1)*65].reshape(-1, p-1, 65)\n",
    "        X_train2 = X_train[:, (p-1)*65:].reshape(-1, (p+1), 65)\n",
    "        X_val1 = X_val[:, :(p-1)*65].reshape(-1, (p-1), 65)\n",
    "        X_val2 = X_val[:, (p-1)*65:].reshape(-1, (p+1), 65)\n",
    "\n",
    "        t = time.time()\n",
    "        vocab1 = MiniBatchKMeans(n_clusters=codebook_size).fit(X_train1.reshape(-1, 65))\n",
    "        codebook1 = vocab1.cluster_centers_\n",
    "        print(\"Kmean =\", time.time()-t)\n",
    "        X_train1 = generate_boaw(X_train1, codebook1)\n",
    "        X_val1 = generate_boaw(X_val1, codebook1)\n",
    "        \n",
    "        vocab2 = MiniBatchKMeans(n_clusters=codebook_size).fit(X_train2.reshape(-1, 65))\n",
    "        codebook2 = vocab2.cluster_centers_\n",
    "        X_train2 = generate_boaw(X_train2, codebook2)\n",
    "        X_val2 = generate_boaw(X_val2, codebook2)\n",
    "\n",
    "        X_train = np.nan_to_num(np.concatenate([X_train1, X_train2], axis=1))  # [num_sample, codebook_size*2]\n",
    "        X_val = np.nan_to_num(np.concatenate([X_val1, X_val2], axis=1))\n",
    "\n",
    "    model = get_model(pos_scale)\n",
    "    model.fit(X_train)\n",
    "\n",
    "    pred = model.decision_function(X_val)  # predict raw outlier scores on test\n",
    "    auc = roc_auc_score(y_val, pred)\n",
    "    preds.append(pred)\n",
    "    aucs.append(auc)\n",
    "    print(auc)\n",
    "    del model\n",
    "\n",
    "targets = np.concatenate(targets)\n",
    "preds = np.concatenate(preds)\n",
    "\n",
    "print(\"(!) cv5 AUC \", np.mean(aucs), np.std(aucs))\n",
    "evaluate(preds, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp \"SoundDr_cough.ipynb\" \"$OUTPUT_DIR/Coswara_cough_frill_svm_1111.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
