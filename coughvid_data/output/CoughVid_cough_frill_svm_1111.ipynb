{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook to keep a benchmarking result of Table 7 on Covid-19 and Abnormal Detection task, include Unsupervised(Isolation Forest, XGBOD) at last this notebook.\n",
    "\n",
    "+ Use CoughVid dataset ```dataset_type='CoughVid'```:\n",
    "+ Use FRILL pretrain model to extract Feature ```PRETRAIN=\"FRILL\"```\n",
    "+ Use SVM method to classify\n",
    "+ Set seed 1111 ```seed=1111```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SoX could not be found!\n",
      "\n",
      "    If you do not have SoX, proceed here:\n",
      "     - - - http://sox.sourceforge.net/ - - -\n",
      "\n",
      "    If you do (or think that you should) have SoX, double-check your\n",
      "    path variables.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from glob import glob\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os, time, math, random, cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile, pickle, h5py, joblib, json\n",
    "import multiprocessing\n",
    "\n",
    "import librosa\n",
    "import opensmile\n",
    "import xgboost as xgb\n",
    "\n",
    "from math import pi\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from scipy.fftpack import fft, hilbert\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict, train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score, auc, precision_recall_curve, roc_curve, average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>fever_muscle_pain</th>\n",
       "      <th>respiratory_condition</th>\n",
       "      <th>status</th>\n",
       "      <th>quality_1</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>label_covid</th>\n",
       "      <th>file_path</th>\n",
       "      <th>label_abnormal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7677</th>\n",
       "      <td>ffbc6bfc-bd8f-4d65-97a7-638d69f42205</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>healthy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.0</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>public_dataset/ffbc6bfc-bd8f-4d65-97a7-638d69f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7678</th>\n",
       "      <td>ffd18a56-096d-40fc-9862-e5c5a8ca1fcd</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>healthy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>public_dataset/ffd18a56-096d-40fc-9862-e5c5a8c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7679</th>\n",
       "      <td>ffd42893-4119-4855-9aad-c67d8d392cc1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>healthy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.0</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>public_dataset/ffd42893-4119-4855-9aad-c67d8d3...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7680</th>\n",
       "      <td>ffedc843-bfc2-4ad6-a749-2bc86bdac84a</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>healthy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.0</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>public_dataset/ffedc843-bfc2-4ad6-a749-2bc86bd...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7681</th>\n",
       "      <td>ffeea120-92a4-40f9-b692-c3865c7a983f</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>healthy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>public_dataset/ffeea120-92a4-40f9-b692-c3865c7...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      uuid fever_muscle_pain  \\\n",
       "7677  ffbc6bfc-bd8f-4d65-97a7-638d69f42205             False   \n",
       "7678  ffd18a56-096d-40fc-9862-e5c5a8ca1fcd             False   \n",
       "7679  ffd42893-4119-4855-9aad-c67d8d392cc1             False   \n",
       "7680  ffedc843-bfc2-4ad6-a749-2bc86bdac84a             False   \n",
       "7681  ffeea120-92a4-40f9-b692-c3865c7a983f             False   \n",
       "\n",
       "     respiratory_condition   status quality_1   age  gender  label_covid  \\\n",
       "7677                 False  healthy       NaN  35.0    male            0   \n",
       "7678                 False  healthy       NaN  25.0  female            0   \n",
       "7679                 False  healthy       NaN  26.0    male            0   \n",
       "7680                 False  healthy       NaN  23.0    male            0   \n",
       "7681                 False  healthy       NaN  22.0  female            0   \n",
       "\n",
       "                                              file_path  label_abnormal  \n",
       "7677  public_dataset/ffbc6bfc-bd8f-4d65-97a7-638d69f...               0  \n",
       "7678  public_dataset/ffd18a56-096d-40fc-9862-e5c5a8c...               0  \n",
       "7679  public_dataset/ffd42893-4119-4855-9aad-c67d8d3...               0  \n",
       "7680  public_dataset/ffedc843-bfc2-4ad6-a749-2bc86bd...               0  \n",
       "7681  public_dataset/ffeea120-92a4-40f9-b692-c3865c7...               0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample rate\n",
    "SR = 44100\n",
    "#100 ms\n",
    "FRAME_LEN = int(SR / 10)\n",
    "#50% overlap, meaning 5ms hop length\n",
    "HOP = int(FRAME_LEN / 2)\n",
    "#the MFCC dimension\n",
    "MFCC_dim = 13\n",
    "PRETRAIN = 'FRILL'\n",
    "codebook_size = 1000\n",
    "\n",
    "fold_num = 5\n",
    "seed = 1111\n",
    "\n",
    "dataset_type = 'CoughVid'\n",
    "\n",
    "if dataset_type == 'SoundDr':\n",
    "    PERIOD = 15\n",
    "    SR = 48000\n",
    "    DIR_DATA = \"./sounddr_data/\"\n",
    "\n",
    "    df = pd.read_csv(DIR_DATA + 'data.csv')\n",
    "    df['file_path'] = df['file_cough'] + '.wav'\n",
    "\n",
    "    df['label_symptom'] = (df['symptoms_status_choice'].map(str) != \"['No']\").astype(int)\n",
    "    df['label_abnormal'] = ((df['symptoms_status_choice'].map(str) != \"['No']\") | (df['cov19_status_choice'] != 'never')).astype(int)\n",
    "    df['label_covid'] = (df['cov19_status_choice'] != 'never').astype(int)\n",
    "elif dataset_type == 'CoughVid':\n",
    "    PERIOD = 10\n",
    "    SR = 22050\n",
    "\n",
    "    DIR_DATA = './coughvid_data/'\n",
    "\n",
    "    VidData   = pd.read_csv(os.path.join(DIR_DATA, 'public_dataset/metadata_compiled.csv'), header=0)\n",
    "    VidData   = VidData.loc[VidData['cough_detected'] >= 0.9][['uuid','fever_muscle_pain','respiratory_condition','status', 'quality_1', 'age', 'gender']]\n",
    "    VidData.dropna(subset=['uuid','fever_muscle_pain','respiratory_condition','status'], inplace=True)\n",
    "    VidData = VidData[(VidData['quality_1'] != 'no_cough') & (VidData['quality_1'] != 'poor')]\n",
    "    VidData = VidData[(VidData['status'] != 'symptomatic') & (VidData['status'].notna())]\n",
    "    VidData['label_covid'] = (VidData['status'] == 'COVID-19').astype(int)\n",
    "\n",
    "    extradata = VidData.loc[VidData['status']=='COVID-19']\n",
    "    notradata = VidData.loc[VidData['status']!='COVID-19']\n",
    "\n",
    "    df = pd.concat([extradata, notradata], ignore_index= True)\n",
    "    df['file_path'] = df['uuid'].apply(lambda x: 'public_dataset/' + x + '.webm')\n",
    "    def g(x):\n",
    "        for i in x:\n",
    "            if i is True:\n",
    "                return 1\n",
    "        return 0\n",
    "    df['label_abnormal'] = df[['fever_muscle_pain', 'respiratory_condition', 'label_covid']].apply(lambda x: g(x), axis=1)\n",
    "else:\n",
    "    PERIOD = 5\n",
    "    SR = 44100\n",
    "    DIR_DATA = \"./coswara_data/\"\n",
    "\n",
    "    join_by = pd.read_csv(os.path.join(DIR_DATA, 'combined_data.csv'))\n",
    "    df_list = []\n",
    "    for each in os.listdir(DIR_DATA):\n",
    "        for path in tqdm(glob(DIR_DATA + each + '/*/cough-shallow.wav')):\n",
    "            temp = pd.DataFrame(columns=['id', 'DIR'])\n",
    "            temp['id'] = [path.split('/')[-2]]\n",
    "            temp['DIR'] = [path]\n",
    "            temp = pd.merge(left=temp,right=join_by,on='id',how='inner')\n",
    "\n",
    "            temp['label_cough'] = (temp['cough'] == True).astype(int)\n",
    "\n",
    "            temp['file_path'] = each + '/' + temp['id'] + '/cough-shallow.wav'\n",
    "            temp['label_covid'] = temp['covid_status'].apply(lambda x: 1 if x == 'positive_mild' or x =='positive_moderate' or x == 'COVID-19' else 0)\n",
    "            df_list.append(temp)\n",
    "    df = pd.concat(df_list)\n",
    "    def g(x):\n",
    "        for i in x:\n",
    "            if i is True:\n",
    "                return 1\n",
    "        return 0\n",
    "    df['label_abnormal'] = df[['st', 'bd', 'cld', 'pneumonia', 'others_resp', 'asthma', 'label_covid']].apply(lambda x: g(x), axis=1)\n",
    "\n",
    "target_col = 'label_abnormal'\n",
    "OUTPUT_DIR = DIR_DATA + 'output/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok = True)\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_duration(filename, mono=True, res_type=\"kaiser_fast\"):\n",
    "    duration = 0\n",
    "    sr = SR\n",
    "    try:\n",
    "        y, sr = librosa.load(filename, sr=None, mono=mono, res_type=res_type)\n",
    "        duration = librosa.get_duration(y=y, sr=sr)\n",
    "    except:\n",
    "        print('Error file:' + filename)\n",
    "    return duration, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6224\n",
       "1    1458\n",
       "Name: label_abnormal, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[target_col].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def crop_or_pad(y, length):\n",
    "    if len(y) < length:\n",
    "        y = np.concatenate([y, np.zeros(length-len(y))])\n",
    "    elif len(y) > length:\n",
    "        cut = random.randint(0, len(y) - length)\n",
    "        y = y[cut:cut+length]\n",
    "    return y\n",
    "\n",
    "def merge_feature(list_features):\n",
    "    \"\"\"\n",
    "      Merge numpy array features\n",
    "      Args:\n",
    "        - list_features: list of numpy array features\n",
    "                         :type: a list of numpy arrays \n",
    "      Returns:\n",
    "        - features: the concatenate numpy array along axis=1\n",
    "                    :type: a numpy array                 \n",
    "    \"\"\"      \n",
    "    features = np.concatenate(list_features, axis=1)\n",
    "    features = np.nan_to_num(features)\n",
    "    features = np.clip(features, -np.finfo(np.float32).max, np.finfo(np.float32).max)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(cfs_matrix):\n",
    "    \"\"\"\n",
    "      Calculate common metrics based on the confusion matrix\n",
    "      Args:\n",
    "        - cfs_matrix: a sklearn confusion matrix \n",
    "                      :type: a ndarray of shape (n_classes, n_classes)\n",
    "      Returns:\n",
    "        - precision: the precision of the prediction\n",
    "                     :type: float  \n",
    "        - recall: the recall of the prediction\n",
    "                  :type: float  \n",
    "        - f1: the f1-score of the prediction\n",
    "              :type: float                       \n",
    "    \"\"\"     \n",
    "    precision = cfs_matrix[1,1] / (cfs_matrix[1,1] + cfs_matrix[0,1])\n",
    "    recall = cfs_matrix[1,1] / (cfs_matrix[1,1] + cfs_matrix[1,0])\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(OUTPUT_DIR + PRETRAIN + '.pickle'):\n",
    "    import tensorflow.compat.v2 as tf\n",
    "    tf.enable_v2_behavior()\n",
    "    import tensorflow_hub as hub\n",
    "\n",
    "    frill_nofrontend_model = hub.load('https://tfhub.dev/google/nonsemantic-speech-benchmark/frill-nofrontend/1')\n",
    "\n",
    "    def stabilized_log(data, additive_offset, floor):\n",
    "      \"\"\"TF version of mfcc_mel.StabilizedLog.\"\"\"\n",
    "      return tf.math.log(tf.math.maximum(data, floor) + additive_offset)\n",
    "\n",
    "\n",
    "    def log_mel_spectrogram(data,\n",
    "                            audio_sample_rate,\n",
    "                            num_mel_bins=64,\n",
    "                            log_additive_offset=0.001,\n",
    "                            log_floor=1e-12,\n",
    "                            window_length_secs=0.025,\n",
    "                            hop_length_secs=0.010,\n",
    "                            fft_length=None):\n",
    "        \"\"\"TF version of mfcc_mel.LogMelSpectrogram.\"\"\"\n",
    "        window_length_samples = int(round(audio_sample_rate * window_length_secs))\n",
    "        hop_length_samples = int(round(audio_sample_rate * hop_length_secs))\n",
    "        if not fft_length:\n",
    "            fft_length = 2 ** int(np.ceil(np.log(window_length_samples) / np.log(2.0)))\n",
    "\n",
    "        spectrogram = tf.abs(\n",
    "            tf.signal.stft(\n",
    "                tf.cast(data, tf.dtypes.float64),\n",
    "                frame_length=window_length_samples,\n",
    "                frame_step=hop_length_samples,\n",
    "                fft_length=fft_length,\n",
    "                window_fn=tf.signal.hann_window,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        to_mel = tf.signal.linear_to_mel_weight_matrix(\n",
    "            num_mel_bins=num_mel_bins,\n",
    "            num_spectrogram_bins=fft_length // 2 + 1,\n",
    "            sample_rate=audio_sample_rate,\n",
    "            lower_edge_hertz=125.0,\n",
    "            upper_edge_hertz=7500.0,\n",
    "            dtype=tf.dtypes.float64\n",
    "        )\n",
    "\n",
    "        mel = spectrogram @ to_mel\n",
    "        log_mel = stabilized_log(mel, log_additive_offset, log_floor)\n",
    "        return log_mel\n",
    "\n",
    "    def compute_frontend_features(samples, sr, frame_hop, n_required=16000, num_mel_bins=64, frame_width=96):\n",
    "        if samples.dtype == np.int16:\n",
    "            samples = tf.cast(samples, np.float32) / np.iinfo(np.int16).max\n",
    "        if samples.dtype == np.float64:\n",
    "            samples = tf.cast(samples, np.float32)\n",
    "        assert samples.dtype == np.float32, samples.dtype\n",
    "        n = tf.size(samples)\n",
    "        samples = tf.cond(\n",
    "            n < n_required,\n",
    "            lambda: tf.pad(samples, [(0, n_required - n)]),\n",
    "            lambda: samples\n",
    "        )\n",
    "        mel = log_mel_spectrogram(samples, sr, num_mel_bins=num_mel_bins)\n",
    "        mel = tf.signal.frame(mel, frame_length=frame_width, frame_step=frame_hop, axis=0)\n",
    "        return mel\n",
    "\n",
    "    def make_nonsemantic_frill_nofrontend_feat(filename):\n",
    "        try:\n",
    "            waveform, _ = librosa.load(os.path.join(DIR_DATA, filename), sr=16000, mono=True, res_type=\"kaiser_fast\")\n",
    "            if 2048 > waveform.shape[-1]:\n",
    "                print('File length < 2048')\n",
    "                return None, filename\n",
    "            frontend_feats = tf.expand_dims(compute_frontend_features(waveform, 16000, frame_hop=17), axis=-1).numpy().astype(np.float32)\n",
    "            assert frontend_feats.shape[1:] == (96, 64, 1)\n",
    "\n",
    "            embeddings = frill_nofrontend_model(frontend_feats)['embedding']\n",
    "            mean_emb = embeddings.numpy().mean(axis=0)\n",
    "            std_emb = embeddings.numpy().std(axis=0)\n",
    "        except Exception as e:\n",
    "            print('Error: ' + str(e))\n",
    "            return None, filename\n",
    "        return np.concatenate((mean_emb, std_emb)), filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_features_of_list_audio(df):\n",
    "    X_features = []\n",
    "    df['error'] = 0\n",
    "\n",
    "    for idx, r in tqdm(df.iterrows(), total=len(df)):\n",
    "        feature, filename = make_nonsemantic_frill_nofrontend_feat(r['file_path'])\n",
    "        \n",
    "        if feature is None:\n",
    "            df['error'][df['file_path'] == filename] = 1\n",
    "        else:\n",
    "            X_features.append(feature)\n",
    "    return np.array(X_features), df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data feature shape: ((7282, 4096), 7282)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(OUTPUT_DIR + PRETRAIN + '.pickle'):\n",
    "    X_features, df = get_features_of_list_audio(df)\n",
    "    df.to_csv(os.path.join(OUTPUT_DIR, 'data.csv'), index=False)\n",
    "    pickle.dump({\n",
    "        'X_trill_features': X_features\n",
    "    }, open(OUTPUT_DIR + PRETRAIN + '.pickle', \"wb\" ))\n",
    "else:\n",
    "    df = pd.read_csv(os.path.join(OUTPUT_DIR, 'data.csv'))\n",
    "    f = pickle.load(open(OUTPUT_DIR + PRETRAIN + '.pickle', \"rb\" ))\n",
    "    X_features = f['X_trill_features']\n",
    "df = df[df['error'] == 0].reset_index(drop=True)\n",
    "\n",
    "# if PRETRAIN == 'OpenSmile':\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(merge_feature([X_features]))\n",
    "# else:\n",
    "# X = merge_feature([X_features])\n",
    "print(f\"Data feature shape: {X.shape, len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(ensem_preds, targets):\n",
    "    \"\"\"\n",
    "      Evaluate the prediction by providing metrics & also the best threshold (to get the highest f1-score)\n",
    "      Ex: AUC, Accurary, Precision, Recall, F1-Score.\n",
    "      Then print these metrics\n",
    "      Args:\n",
    "        - ensem_preds: predictions for ids \n",
    "                       :type: a numpy array\n",
    "        - targets: the actual results of ids \n",
    "                   :type: a numpy array                 \n",
    "      Returns:\n",
    "        - None                  \n",
    "    \"\"\"     \n",
    "    best_th = 0\n",
    "    best_score = 0\n",
    "\n",
    "    for th in np.arange(0.0, 0.6, 0.01):\n",
    "        pred = (ensem_preds > th).astype(int)\n",
    "        score = f1_score(targets, pred)\n",
    "        if score > best_score:\n",
    "            best_th = th\n",
    "            best_score = score\n",
    "\n",
    "    print(f\"\\nAUC score: {roc_auc_score(targets, ensem_preds):12.4f}\")\n",
    "    print(f\"Best threshold {best_th:12.4f}\")\n",
    "\n",
    "    preds = (ensem_preds > best_th).astype(int)\n",
    "\n",
    "    cm1 = confusion_matrix(targets, preds)\n",
    "    print('\\nConfusion Matrix : \\n', cm1)\n",
    "    precision, recall, f1 = compute_metrics(cm1)\n",
    "    \n",
    "    print('\\n=============')\n",
    "    print (f'Precision    : {precision:12.4f}')\n",
    "    \n",
    "    print(f'Recall : {recall:12.4f}')\n",
    "    \n",
    "    print(f'F1 Score : {f1:12.4f}')\n",
    "    \n",
    "    total1=sum(sum(cm1))\n",
    "\n",
    "    print('\\n=============')\n",
    "    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
    "    print (f'Accuracy    : {accuracy1:12.4f}')\n",
    "\n",
    "def get_model(pos_scale, c=1):\n",
    "    model = LinearSVC(C=c, class_weight='balanced', random_state=seed)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train COVID-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    " if not os.path.exists(OUTPUT_DIR + 'df_label_covid_5fold.csv'):\n",
    "    folds = df.copy()\n",
    "    Fold = StratifiedKFold(n_splits=fold_num, shuffle=True, random_state=seed)\n",
    "    for n, (train_index, val_index) in enumerate(Fold.split(folds, folds['label_covid'])):\n",
    "        folds.loc[val_index, 'fold'] = int(n)\n",
    "    folds['fold'] = folds['fold'].astype(int)\n",
    "    folds.to_csv(OUTPUT_DIR + 'df_label_covid_5fold.csv', index=False)\n",
    "else:\n",
    "    folds = pd.read_csv(OUTPUT_DIR + 'df_label_covid_5fold.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.726247987117553\n",
      "0.5561861861861862\n",
      "0.5427606901725431\n",
      "0.5432771481158578\n",
      "0.548338661241887\n",
      "0.5369926377990895\n",
      "(!) cv5 AUC  0.5455110647031127 0.006435403035721559\n",
      "\n",
      "AUC score:       0.5455\n",
      "Best threshold       0.0000\n",
      "\n",
      "Confusion Matrix : \n",
      " [[5798  863]\n",
      " [ 484  137]]\n",
      "\n",
      "=============\n",
      "Precision    :       0.1370\n",
      "Recall :       0.2206\n",
      "F1 Score :       0.1690\n",
      "\n",
      "=============\n",
      "Accuracy    :       0.8150\n"
     ]
    }
   ],
   "source": [
    "y = folds['label_covid']\n",
    "pos_scale = (y == 0).sum() / (y == 1).sum()\n",
    "print(pos_scale)\n",
    "targets = []\n",
    "preds = []\n",
    "aucs = []\n",
    "\n",
    "for fold in range(5):\n",
    "    train_idx = folds['fold'] != fold\n",
    "    valid_idx = folds['fold'] == fold\n",
    "    X_train = X[train_idx]\n",
    "    y_train = y[train_idx]\n",
    "    X_val = X[valid_idx]\n",
    "    y_val = y[valid_idx]\n",
    "\n",
    "    targets.append(y_val)\n",
    "    model = get_model(pos_scale=pos_scale)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_val)\n",
    "    preds.append(pred)\n",
    "    auc = roc_auc_score(y_val, pred)\n",
    "    aucs.append(auc)\n",
    "    print(auc)\n",
    "    del model\n",
    "\n",
    "targets = np.concatenate(targets)\n",
    "preds = np.concatenate(preds)\n",
    "\n",
    "print(\"(!) cv5 AUC \", np.mean(aucs), np.std(aucs))\n",
    "evaluate(preds, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train abnormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    " if not os.path.exists(OUTPUT_DIR + 'df_label_abnormal_5fold.csv'):\n",
    "    folds = df.copy()\n",
    "    Fold = StratifiedKFold(n_splits=fold_num, shuffle=True, random_state=seed)\n",
    "    for n, (train_index, val_index) in enumerate(Fold.split(folds, folds['label_abnormal'])):\n",
    "        folds.loc[val_index, 'fold'] = int(n)\n",
    "    folds['fold'] = folds['fold'].astype(int)\n",
    "    folds.to_csv(OUTPUT_DIR + 'df_label_abnormal_5fold.csv', index=False)\n",
    "else:\n",
    "    folds = pd.read_csv(OUTPUT_DIR + 'df_label_abnormal_5fold.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.190306486101211\n",
      "0.5127702326482194\n",
      "0.5146282712373205\n",
      "0.5030612244897958\n",
      "0.507142857142857\n",
      "0.5087801923222534\n",
      "(!) cv5 AUC  0.5092765555680893 0.004105509127963959\n",
      "\n",
      "AUC score:       0.5093\n",
      "Best threshold       0.0000\n",
      "\n",
      "Confusion Matrix : \n",
      " [[5787   92]\n",
      " [1355   48]]\n",
      "\n",
      "=============\n",
      "Precision    :       0.3429\n",
      "Recall :       0.0342\n",
      "F1 Score :       0.0622\n",
      "\n",
      "=============\n",
      "Accuracy    :       0.8013\n"
     ]
    }
   ],
   "source": [
    "y = folds['label_abnormal']\n",
    "pos_scale = (y == 0).sum() / (y == 1).sum()\n",
    "print(pos_scale)\n",
    "targets = []\n",
    "preds = []\n",
    "aucs = []\n",
    "\n",
    "def get_model(pos_scale):\n",
    "    model = xgb.XGBClassifier(\n",
    "        max_depth=7,\n",
    "        scale_pos_weight=pos_scale,\n",
    "        learning_rate=0.3,\n",
    "        n_estimators=200,\n",
    "        subsample=1,\n",
    "        colsample_bytree=1,\n",
    "        nthread=-1,\n",
    "        seed=seed,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    return model\n",
    "\n",
    "for fold in range(5):\n",
    "    train_idx = folds['fold'] != fold\n",
    "    valid_idx = folds['fold'] == fold\n",
    "    X_train = X[train_idx]\n",
    "    y_train = y[train_idx]\n",
    "    X_val = X[valid_idx]\n",
    "    y_val = y[valid_idx]\n",
    "\n",
    "    targets.append(y_val)\n",
    "    model = get_model(pos_scale=pos_scale)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_val)\n",
    "    preds.append(pred)\n",
    "    auc = roc_auc_score(y_val, pred)\n",
    "    aucs.append(auc)\n",
    "    print(auc)\n",
    "    del model\n",
    "\n",
    "targets = np.concatenate(targets)\n",
    "preds = np.concatenate(preds)\n",
    "\n",
    "print(\"(!) cv5 AUC \", np.mean(aucs), np.std(aucs))\n",
    "evaluate(preds, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test other model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.190306486101211\n"
     ]
    }
   ],
   "source": [
    "y = folds['label_abnormal']\n",
    "pos_scale = (y == 0).sum() / (y == 1).sum()\n",
    "print(pos_scale)\n",
    "\n",
    "def get_model(pos_scale):\n",
    "    model = IsolationForest(n_estimators=500, max_samples='auto', contamination=0.1, n_jobs=-1, random_state=seed) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5005114145302249\n",
      "0.4987532379499843\n",
      "0.49482507288629735\n",
      "0.5046313168124392\n",
      "0.47633527674717957\n",
      "(!) cv5 AUC  0.495011263785225 0.009855217988557596\n",
      "\n",
      "AUC score:       0.4952\n",
      "Best threshold       0.0000\n",
      "\n",
      "Confusion Matrix : \n",
      " [[5307  572]\n",
      " [1237  166]]\n",
      "\n",
      "=============\n",
      "Precision    :       0.2249\n",
      "Recall :       0.1183\n",
      "F1 Score :       0.1551\n",
      "\n",
      "=============\n",
      "Accuracy    :       0.7516\n"
     ]
    }
   ],
   "source": [
    "targets = []\n",
    "preds = []\n",
    "aucs = []\n",
    "\n",
    "for fold in range(5):\n",
    "    train_idx = folds['fold'] != fold\n",
    "    valid_idx = folds['fold'] == fold\n",
    "    X_train = X[train_idx]\n",
    "    y_train = y[train_idx]\n",
    "    X_val = X[valid_idx]\n",
    "    y_val = y[valid_idx]\n",
    "\n",
    "    targets.append(y_val)\n",
    "    model = get_model(pos_scale)\n",
    "    model.fit(X_train)\n",
    "\n",
    "    scores = (-1.0) * model.decision_function(X_val)\n",
    "    pred = scores.flatten()\n",
    "    preds.append(pred)\n",
    "    auc = roc_auc_score(y_val, pred)\n",
    "    aucs.append(auc)\n",
    "    print(auc)\n",
    "    del model\n",
    "\n",
    "targets = np.concatenate(targets)\n",
    "preds = np.concatenate(preds)\n",
    "\n",
    "print(\"(!) cv5 AUC \", np.mean(aucs), np.std(aucs))\n",
    "evaluate(preds, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.190306486101211\n"
     ]
    }
   ],
   "source": [
    "y = folds['label_abnormal']\n",
    "pos_scale = (y == 0).sum() / (y == 1).sum()\n",
    "print(pos_scale)\n",
    "\n",
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "def get_model(pos_scale):\n",
    "    # Model candidate\n",
    "    model = XGBOD(max_depth=7,\n",
    "        scale_pos_weight=pos_scale,\n",
    "        learning_rate=0.3,\n",
    "        n_estimators=200,\n",
    "        subsample=1,\n",
    "        colsample_bytree=1,\n",
    "        nthread=-1,\n",
    "        seed=seed,\n",
    "        eval_metric='logloss')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:33:55] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "0.5851096666424578\n",
      "[16:34:31] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "0.5898304161522261\n",
      "[18:34:45] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "0.5942753887269193\n",
      "[20:34:56] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "0.5900115403304179\n",
      "[22:35:12] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "0.5820034830014387\n",
      "(!) cv5 AUC  0.5882460989706919 0.004261141654065307\n",
      "\n",
      "AUC score:       0.5885\n",
      "Best threshold       0.0100\n",
      "\n",
      "Confusion Matrix : \n",
      " [[2661 3218]\n",
      " [ 460  943]]\n",
      "\n",
      "=============\n",
      "Precision    :       0.2266\n",
      "Recall :       0.6721\n",
      "F1 Score :       0.3390\n",
      "\n",
      "=============\n",
      "Accuracy    :       0.4949\n"
     ]
    }
   ],
   "source": [
    "targets = []\n",
    "preds = []\n",
    "aucs = []\n",
    "\n",
    "for fold in range(5):\n",
    "    train_idx = folds['fold'] != fold\n",
    "    valid_idx = folds['fold'] == fold\n",
    "    X_train = X[train_idx]\n",
    "    y_train = y[train_idx]\n",
    "    X_val = X[valid_idx]\n",
    "    y_val = y[valid_idx]\n",
    "\n",
    "    targets.append(y_val)\n",
    "    model = get_model(pos_scale)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    pred = model.decision_function(X_val)  # predict raw outlier scores on test\n",
    "    auc = roc_auc_score(y_val, pred)\n",
    "    preds.append(pred)\n",
    "    aucs.append(auc)\n",
    "    print(auc)\n",
    "    del model\n",
    "\n",
    "targets = np.concatenate(targets)\n",
    "preds = np.concatenate(preds)\n",
    "\n",
    "print(\"(!) cv5 AUC \", np.mean(aucs), np.std(aucs))\n",
    "evaluate(preds, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.190306486101211\n"
     ]
    }
   ],
   "source": [
    "y = folds['label_abnormal']\n",
    "pos_scale = (y == 0).sum() / (y == 1).sum()\n",
    "print(pos_scale)\n",
    "\n",
    "from pyod.models.iforest import IForest\n",
    "\n",
    "def get_model(pos_scale):\n",
    "    model = IForest(n_estimators=500, max_samples='auto', contamination=0.1, n_jobs=-1, random_state=seed) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5005114145302249\n",
      "0.4987532379499843\n",
      "0.49482507288629735\n",
      "0.5046313168124392\n",
      "0.47633527674717957\n",
      "(!) cv5 AUC  0.495011263785225 0.009855217988557596\n",
      "\n",
      "AUC score:       0.4952\n",
      "Best threshold       0.0000\n",
      "\n",
      "Confusion Matrix : \n",
      " [[5307  572]\n",
      " [1237  166]]\n",
      "\n",
      "=============\n",
      "Precision    :       0.2249\n",
      "Recall :       0.1183\n",
      "F1 Score :       0.1551\n",
      "\n",
      "=============\n",
      "Accuracy    :       0.7516\n"
     ]
    }
   ],
   "source": [
    "targets = []\n",
    "preds = []\n",
    "aucs = []\n",
    "\n",
    "for fold in range(5):\n",
    "    train_idx = folds['fold'] != fold\n",
    "    valid_idx = folds['fold'] == fold\n",
    "    X_train = X[train_idx]\n",
    "    y_train = y[train_idx]\n",
    "    X_val = X[valid_idx]\n",
    "    y_val = y[valid_idx]\n",
    "\n",
    "    targets.append(y_val)\n",
    "    model = get_model(pos_scale)\n",
    "    model.fit(X_train)\n",
    "\n",
    "    pred = model.decision_function(X_val)  # predict raw outlier scores on test\n",
    "    auc = roc_auc_score(y_val, pred)\n",
    "    preds.append(pred)\n",
    "    aucs.append(auc)\n",
    "    print(auc)\n",
    "    del model\n",
    "\n",
    "targets = np.concatenate(targets)\n",
    "preds = np.concatenate(preds)\n",
    "\n",
    "print(\"(!) cv5 AUC \", np.mean(aucs), np.std(aucs))\n",
    "evaluate(preds, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp \"SoundDr_cough.ipynb\" \"$OUTPUT_DIR/CoughVid_cough_frill_svm_1111.ipynb\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
